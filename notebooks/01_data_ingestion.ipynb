{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\MyOnlineCourses\\\\ML_Projects\\\\arabic-digits-recognition'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_path: str\n",
    "    data_file: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adr.constants import *\n",
    "from adr.utils.help import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_path=config.source_path,\n",
    "            data_file=config.data_file,\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config, self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-13 17:07:07,101: INFO: help: yaml file: params.yaml loaded successfully. Content size: 9]\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Union, IO\n",
    "import pathlib, os\n",
    "import random\n",
    "import numpy as np \n",
    "from src.adr.utils.dataset import SeqDataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from src.adr.utils.common import convert_digits, get_wav_path, get_wav_label, ArdArray, Signal\n",
    "from src.adr.utils.preprocess import WVLoader, MFCCExtractor\n",
    "from src.adr.utils.transformer import  MFCC, MinMaxScaler, Standardiser, TransformsChain\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from adr import logger\n",
    "\n",
    "class DataIngestion:\n",
    "    \n",
    "    def __init__(self,\n",
    "                config : DataIngestionConfig,\n",
    "                params : None\n",
    "                \n",
    "                 ):\n",
    "        \n",
    "        self.loader = WVLoader()\n",
    "        self.extractor= MFCCExtractor()\n",
    "        self.params=params\n",
    "        self._target_sample_rate = self.params.TARGET_SAMPLE_RATE\n",
    "        self._num_samples = self.params.NUM_SAMPLES\n",
    "        self._random_state = self.params.RANDOM_STATE\n",
    "        self._transform_kwargs = self.params.SPEC_KWARGS\n",
    "        self.config=config\n",
    "        self._source_path = self.config.source_path\n",
    "        self._data_file = self.config.data_file\n",
    "        self.minmax_scaler = MinMaxScaler(min=0, max=1)\n",
    "        self.standardiser = Standardiser()\n",
    "        self.transform_chain = TransformsChain(transforms=[self.standardiser])\n",
    "\n",
    "        assert self._target_sample_rate > 0, \"Sample rate must be a positive integer\"\n",
    "    \n",
    "    def Load(self): \n",
    "        files_list, _inputs, _targets, _lengths = [], [],[], []\n",
    "        classes = range(10)\n",
    "        for file in os.listdir(self._source_path):\n",
    "            if file.lower().endswith(\".wav\"):\n",
    "                files_list.append(file)\n",
    "        \n",
    "        if files_list:\n",
    "            random.shuffle(files_list)\n",
    "            \n",
    "        with tqdm(total=len(files_list), colour=\"green\", desc=\"Processing MFCC \", \n",
    "                  bar_format=\"{l_bar}{bar} [time spent: {elapsed}]\",\n",
    "                  leave=True) as pbar:\n",
    "            for file_name in files_list:\n",
    "                wav_path = get_wav_path(file_name, self._source_path)\n",
    "                label = get_wav_label(file_name)\n",
    "                waveform, sr = self.loader.load(file = wav_path)\n",
    "                mfcc_signal = self.extractor.mfcc(audio=waveform, sample_rate=sr)\n",
    "                signal = Signal(name = file_name.split('\\\\')[-1], data=mfcc_signal, samplerate=self._target_sample_rate, filepath=wav_path)\n",
    "                signal = self.transform_chain.process(signal)\n",
    "                # Append the MFCC features to the list\n",
    "              \n",
    "               \n",
    "                _inputs.append(signal.data)\n",
    "                _targets.append(label)\n",
    "                _lengths.append(signal.data.shape[0])\n",
    "                pbar.update(1)\n",
    "                time.sleep(0.01)\n",
    "        max_length = max(mfcc.shape[1] for mfcc in _inputs) \n",
    "        sequences = pad_sequences([mfcc.T for mfcc in _inputs], maxlen=max_length, padding='post', dtype='float32')\n",
    "        logger.info(f\"Padded MFCC shape: {sequences.shape}\")\n",
    "        logger.info(f\"Labels shape: {np.array(_targets).shape}\")          \n",
    "        #sequences = np.array(_inputs, dtype=object)\n",
    "        lengths = np.array(_lengths, dtype=int)\n",
    "        idx = np.argwhere(np.isin(_targets, classes)).flatten()\n",
    "        return SeqDataset(features= sequences[idx], targets = np.array(_targets)[idx],\n",
    "                          lengths = lengths[idx], classes = classes, path =self._data_file, \n",
    "                          random_state = self._random_state)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-13 17:07:07,542: INFO: help: yaml file: config\\config.yaml loaded successfully. Content size: 9]\n",
      "[2024-08-13 17:07:07,551: INFO: help: yaml file: params.yaml loaded successfully. Content size: 9]\n",
      "[2024-08-13 17:07:07,555: INFO: help: Total directories created: 1]\n",
      "[2024-08-13 17:07:07,559: INFO: help: Total directories created: 1]\n",
      "[2024-08-13 17:07:07,561: INFO: preprocess: WVLoader is initializing]\n",
      "[2024-08-13 17:07:07,563: INFO: transformer: Instantiated TransformType.MINMAXSCALER transform]\n",
      "[2024-08-13 17:07:07,565: INFO: transformer: Instantiated TransformType.STANDARDSCALER transform]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MFCC : 100%|\u001b[32m██████████\u001b[0m [time spent: 00:25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-13 17:07:32,655: INFO: 2883497174: Padded MFCC shape: (402, 52, 40)]\n",
      "[2024-08-13 17:07:32,658: INFO: 2883497174: Labels shape: (402,)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-13 17:07:32,858: INFO: dataset: A npz file has been saved]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config, data_ingestion_params = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config, params=data_ingestion_params)\n",
    "    dataset = data_ingestion.Load()\n",
    "    dataset.save(compress=True)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 402\n",
      "Number of classes: 10\n",
      "Train set size: 321\n",
      "Test set size: 81\n",
      "Sample 0:\n",
      "  Feature shape: (52, 40)\n",
      "  Target: 5\n",
      "  Length: 40\n",
      "Sample 1:\n",
      "  Feature shape: (52, 40)\n",
      "  Target: 8\n",
      "  Length: 40\n",
      "Sample 2:\n",
      "  Feature shape: (52, 40)\n",
      "  Target: 9\n",
      "  Length: 40\n",
      "Class 0: 1 samples\n",
      "Class 1: 1 samples\n",
      "Class 2: 1 samples\n",
      "Class 3: 1 samples\n",
      "Class 4: 1 samples\n",
      "Class 5: 1 samples\n",
      "Class 6: 1 samples\n",
      "Class 7: 1 samples\n",
      "Class 8: 1 samples\n",
      "Class 9: 1 samples\n"
     ]
    }
   ],
   "source": [
    "# Check the dataset\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of classes: {len(dataset._classes)}\")\n",
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = dataset.split_data(split_size=0.2, shuffle=True, stratify=True)\n",
    "\n",
    "print(f\"Train set size: {len(train_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "\n",
    "# Iterate through the dataset\n",
    "for i, (feature, target, length) in enumerate(dataset):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  Feature shape: {feature.shape}\")\n",
    "    print(f\"  Target: {target}\")\n",
    "    print(f\"  Length: {length}\")\n",
    "    if i == 2:  # Print only first 3 samples\n",
    "        break\n",
    "class_samples = {}\n",
    "for features, class_label in dataset.iterator():\n",
    "    if class_label not in class_samples:\n",
    "        class_samples[class_label] = 0\n",
    "    class_samples[class_label] += 1\n",
    "\n",
    "for class_label, count in class_samples.items():\n",
    "    print(f\"Class {class_label}: {count} samples\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('arabdigs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd9d8875aaa423a5faaee251418e522698c11b85bd3df211ca48675ae00acaaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
