{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\MyOnlineCourses\\\\ML_Projects\\\\arabic-digits-recognition'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_path: str\n",
    "    metadata_file: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ard.constants import *\n",
    "from src.ard.utils.help import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_path=config.source_path,\n",
    "            metadata_file=config.metadata_file,\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config, self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, IO\n",
    "import pathlib, os\n",
    "import random\n",
    "#from pydantic import confloat, PositiveInt, Field\n",
    "import numpy as np \n",
    "from src.ard.utils.dataset import SeqDataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from src.ard.utils.common import convert_digits, get_wav_path, get_wav_label, ArdArray, Signal\n",
    "from src.ard.utils.preprocess import WVLoader, MFCCExtractor\n",
    "from src.ard.utils.transformer import  MFCC, MinMaxScaler, Standardiser, TransformsChain\n",
    "\n",
    "\n",
    "class DataIngestion:\n",
    "    \n",
    "    def __init__(self,\n",
    "                config : DataIngestionConfig,\n",
    "                params : None\n",
    "                \n",
    "                 ):\n",
    "        \n",
    "        self.loader = WVLoader()\n",
    "        self.extractor= MFCCExtractor()\n",
    "        self.params=params\n",
    "        self._target_sample_rate = self.params.TARGET_SAMPLE_RATE\n",
    "        self._num_samples = self.params.NUM_SAMPLES\n",
    "        self._random_state = self.params.RANDOM_STATE\n",
    "        self._transform_kwargs = self.params.SPEC_KWARGS\n",
    "        self.config=config\n",
    "        self._source_path = self.config.source_path\n",
    "        self._metadata_file = self.config.metadata_file\n",
    "        self.minmax_scaler = MinMaxScaler(min=0, max=1)\n",
    "        self.standardiser = Standardiser()\n",
    "        self.transform_chain = TransformsChain(transforms=[self.minmax_scaler, self.standardiser])\n",
    "\n",
    "        assert self._target_sample_rate > 0, \"Sample rate must be a positive integer\"\n",
    "    \n",
    "    def Load(self): \n",
    "        files_list, _inputs, _targets, _lengths = [], [],[], []\n",
    "        classes = range(10)\n",
    "        for file in os.listdir(self._source_path):\n",
    "            if file.lower().endswith(\".wav\"):\n",
    "                files_list.append(file)\n",
    "        \n",
    "        if files_list:\n",
    "            random.shuffle(files_list)\n",
    "        with tqdm(total=len(files_list), colour=\"green\", desc=\"Processing MFCC \", \n",
    "                  bar_format=\"{l_bar}{bar} [time spent: {elapsed}]\",\n",
    "                  leave=True) as pbar:\n",
    "            for file_name in files_list:\n",
    "                wav_path = get_wav_path(file_name, self._source_path)\n",
    "                label = get_wav_label(file_name)\n",
    "                waveform = self.loader.load(file = wav_path,sample_rate= self._target_sample_rate)\n",
    "                mfcc_signal = self.extractor.mfcc(data=waveform, spec_kwargs=self._transform_kwargs)\n",
    "                signal = Signal(name = file_name.split('\\\\')[-1], data=mfcc_signal, samplerate=self._target_sample_rate, filepath=wav_path)\n",
    "                scaled_signal = self.transform_chain.process(signal)\n",
    "                _inputs.append(scaled_signal.data)\n",
    "                _targets.append(label)\n",
    "                _lengths.append(scaled_signal.data.shape[0])\n",
    "                pbar.update(1)\n",
    "                time.sleep(0.01)\n",
    "        sequences = np.array(_inputs, dtype=object)\n",
    "        lengths = np.array(_lengths, dtype=int)\n",
    "        idx = np.argwhere(np.isin(_targets, classes)).flatten()\n",
    "        \n",
    "        return SeqDataset(features= np.vstack(sequences[idx]), targets = np.array(_targets)[idx],\n",
    "                          lengths = lengths[idx], classes = classes, path =self._metadata_file, \n",
    "                          random_state = self._random_state)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-07 08:55:48,172: INFO: help: yaml file: config\\config.yaml loaded successfully. Content size: 3]\n",
      "[2024-08-07 08:55:48,180: INFO: help: yaml file: params.yaml loaded successfully. Content size: 6]\n",
      "[2024-08-07 08:55:48,184: INFO: help: Total directories created: 1]\n",
      "[2024-08-07 08:55:48,187: INFO: help: Total directories created: 1]\n",
      "[2024-08-07 08:55:48,191: INFO: preprocess: WVLoader is initializing]\n",
      "[2024-08-07 08:55:48,193: INFO: transformer: Instantiated TransformType.MINMAXSCALER transform]\n",
      "[2024-08-07 08:55:48,197: INFO: transformer: Instantiated TransformType.STANDARDSCALER transform]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MFCC : 100%|\u001b[32m██████████\u001b[0m [time spent: 00:30]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config, data_ingestion_params = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config, params=data_ingestion_params)\n",
    "    dataset = data_ingestion.Load()\n",
    "    dataset.save(compress=True)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 402\n",
      "Number of classes: 10\n",
      "Train set size: 321\n",
      "Test set size: 81\n",
      "Sample 0:\n",
      "  Feature shape: (13,)\n",
      "  Target: 5\n",
      "  Length: 344\n",
      "Sample 1:\n",
      "  Feature shape: (13,)\n",
      "  Target: 0\n",
      "  Length: 270\n",
      "Sample 2:\n",
      "  Feature shape: (13,)\n",
      "  Target: 4\n",
      "  Length: 258\n",
      "Class 5: 32 samples\n",
      "Class 0: 48 samples\n",
      "Class 4: 40 samples\n",
      "Class 3: 38 samples\n",
      "Class 7: 40 samples\n",
      "Class 6: 41 samples\n",
      "Class 2: 38 samples\n",
      "Class 9: 43 samples\n",
      "Class 1: 51 samples\n",
      "Class 8: 31 samples\n"
     ]
    }
   ],
   "source": [
    "# Check the dataset\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of classes: {len(dataset._classes)}\")\n",
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = dataset.split_data(split_size=0.2, shuffle=True, stratify=True)\n",
    "\n",
    "print(f\"Train set size: {len(train_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")\n",
    "\n",
    "# Iterate through the dataset\n",
    "for i, (feature, target, length) in enumerate(dataset):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  Feature shape: {feature.shape}\")\n",
    "    print(f\"  Target: {target}\")\n",
    "    print(f\"  Length: {length}\")\n",
    "    if i == 2:  # Print only first 3 samples\n",
    "        break\n",
    "class_samples = {}\n",
    "for features, class_label in dataset.iterator():\n",
    "    if class_label not in class_samples:\n",
    "        class_samples[class_label] = 0\n",
    "    class_samples[class_label] += 1\n",
    "\n",
    "for class_label, count in class_samples.items():\n",
    "    print(f\"Class {class_label}: {count} samples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,targets,lengths = test_data._get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.3101428 ,  2.6180587 , -1.2077682 ,  0.7328182 ,  0.11773837,\n",
       "       -0.403541  ,  0.12819949, -0.4718498 ,  0.14588565,  0.38098323,\n",
       "       -0.23878922,  0.12566495,  0.23440695], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('ardgrec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdbcc3c2497f60563d94bcb4564b12b6e1b48b054cf59e9281048fbbc05aeb9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
